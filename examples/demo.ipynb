{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# An usage example of NaturalCC\n",
    "\n",
    "Task: Code Completion <br>\n",
    "Dataset: Ruby dataset of [CodeXGLUE (Feng et. al., 2020)](https://arxiv.org/pdf/2002.08155.pdf) <br>\n",
    "Model: SeqRNN <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1. Download dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[2021-11-27 23:35:44]    INFO >> Dataset has been downloaded at /data/ncc_data/demo/Cleaned_CodeSearchNet.zip (4173055814.py:14, <module>())\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gdown\n",
    "\n",
    "from ncc import LOGGER\n",
    "from ncc import __NCC_DIR__\n",
    "from ncc.utils.path_manager import PathManager\n",
    "\n",
    "# CodeSearchNet(feng) dataset\n",
    "DATASET_DIR = os.path.join(__NCC_DIR__, \"demo\")\n",
    "DATASET_URL = \"https://drive.google.com/uc?id=1rd2Tc6oUWBo7JouwexW3ksQ0PaOhUr6h\"\n",
    "out_file = os.path.join(DATASET_DIR, \"Cleaned_CodeSearchNet.zip\")\n",
    "if not PathManager.exists(out_file):\n",
    "    gdown.download(DATASET_URL, output=out_file)\n",
    "LOGGER.info(f\"Dataset has been downloaded at {out_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Step 2. Pre-processing dataset and save it into MMAP format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "### 1) inflate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[2021-11-27 23:35:55]    INFO >> Inflating data at /data/ncc_data/demo (2210737368.py:7, <module>())\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "\n",
    "DATA_DIR = os.path.join(DATASET_DIR, \"completion\")\n",
    "with zipfile.ZipFile(out_file, \"r\") as writer:\n",
    "    writer.extractall(path=DATASET_DIR)\n",
    "\n",
    "LOGGER.info(f\"Inflating data at {DATASET_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) use CodeBERT BPE dictionary to tokenize CodeSearchNet(feng)-ruby codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "#### load CodeBERT BPE dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ncc.data.dictionary import TransformersDictionary\n",
    "\n",
    "vocab = TransformersDictionary.from_pretrained(\"microsoft/codebert-base\", do_lower_case=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tokenization & dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                 | 0/24927 [00:00<?, ?it/s]\u001B[32m[2021-11-27 23:36:14]    INFO >> Show a case of /data/ncc_data/demo/CodeSearchNet/ruby/train.jsonl (1291166181.py:26, <module>())\u001B[0m\n",
      "\u001B[32m[2021-11-27 23:36:14]    INFO >> Before BPE, '['def', 'render_body', '(', 'context', ',', 'options', ')', 'if', 'options', '.', 'key?', '(', ':partial', ')', '[', 'render_partial', '(', 'context', ',', 'options', ')', ']', 'else', 'StreamingTemplateRenderer', '.', 'new', '(', '@lookup_context', ')', '.', 'render', '(', 'context', ',', 'options', ')', 'end', 'end']' (1291166181.py:27, <module>())\u001B[0m\n",
      "\u001B[32m[2021-11-27 23:36:14]    INFO >> After BPE, '['def', 'Ġrender', '_', 'body', 'Ġ(', 'Ġcontext', 'Ġ,', 'Ġoptions', 'Ġ)', 'Ġif', 'Ġoptions', 'Ġ.', 'Ġkey', '?', 'Ġ(', 'Ġ:', 'partial', 'Ġ)', 'Ġ[', 'Ġrender', '_', 'partial', 'Ġ(', 'Ġcontext', 'Ġ,', 'Ġoptions', 'Ġ)', 'Ġ]', 'Ġelse', 'ĠStreaming', 'Template', 'R', 'end', 'erer', 'Ġ.', 'Ġnew', 'Ġ(', 'Ġ@', 'look', 'up', '_', 'context', 'Ġ)', 'Ġ.', 'Ġrender', 'Ġ(', 'Ġcontext', 'Ġ,', 'Ġoptions', 'Ġ)', 'Ġend', 'Ġend']' (1291166181.py:28, <module>())\u001B[0m\n",
      "  0%|▏                                      | 88/24927 [00:00<00:56, 442.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 24927/24927 [00:58<00:00, 423.19it/s]\n",
      "  0%|                                                  | 0/1400 [00:00<?, ?it/s]\u001B[32m[2021-11-27 23:37:13]    INFO >> Show a case of /data/ncc_data/demo/CodeSearchNet/ruby/valid.jsonl (1291166181.py:26, <module>())\u001B[0m\n",
      "\u001B[32m[2021-11-27 23:37:13]    INFO >> Before BPE, '['def', 'preparse', '(', 'unparsed', ',', 'args', '=', '[', ']', ',', 'opts', '=', '{', '}', ')', 'case', 'unparsed', 'when', 'Hash', 'then', 'opts', '.', 'merge!', 'unparsed', 'when', 'Array', 'then', 'unparsed', '.', 'each', '{', '|', 'e', '|', 'preparse', '(', 'e', ',', 'args', ',', 'opts', ')', '}', 'else', 'args', '<<', 'unparsed', '.', 'to_s', 'end', '[', 'args', ',', 'opts', ']', 'end']' (1291166181.py:27, <module>())\u001B[0m\n",
      "\u001B[32m[2021-11-27 23:37:13]    INFO >> After BPE, '['def', 'Ġprepar', 'se', 'Ġ(', 'Ġunp', 'ars', 'ed', 'Ġ,', 'Ġargs', 'Ġ=', 'Ġ[', 'Ġ]', 'Ġ,', 'Ġopt', 's', 'Ġ=', 'Ġ{', 'Ġ}', 'Ġ)', 'Ġcase', 'Ġunp', 'ars', 'ed', 'Ġwhen', 'ĠHash', 'Ġthen', 'Ġopt', 's', 'Ġ.', 'Ġmerge', '!', 'Ġunp', 'ars', 'ed', 'Ġwhen', 'ĠArray', 'Ġthen', 'Ġunp', 'ars', 'ed', 'Ġ.', 'Ġeach', 'Ġ{', 'Ġ|', 'Ġe', 'Ġ|', 'Ġprepar', 'se', 'Ġ(', 'Ġe', 'Ġ,', 'Ġargs', 'Ġ,', 'Ġopt', 's', 'Ġ)', 'Ġ}', 'Ġelse', 'Ġargs', 'Ġ<<', 'Ġunp', 'ars', 'ed', 'Ġ.', 'Ġto', '_', 's', 'Ġend', 'Ġ[', 'Ġargs', 'Ġ,', 'Ġopt', 's', 'Ġ]', 'Ġend']' (1291166181.py:28, <module>())\u001B[0m\n",
      "  3%|█▏                                      | 43/1400 [00:00<00:03, 429.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 1400/1400 [00:03<00:00, 433.57it/s]\n",
      "  0%|                                                  | 0/1261 [00:00<?, ?it/s]\u001B[32m[2021-11-27 23:37:16]    INFO >> Show a case of /data/ncc_data/demo/CodeSearchNet/ruby/test.jsonl (1291166181.py:26, <module>())\u001B[0m\n",
      "\u001B[32m[2021-11-27 23:37:16]    INFO >> Before BPE, '['def', 'print_summary', '(', 'status', ')', 'status_string', '=', 'status', '.', 'to_s', '.', 'humanize', '.', 'upcase', 'if', 'status', '==', ':success', 'heading', '(', '\"Result: \"', ',', 'status_string', ',', ':green', ')', 'level', '=', ':info', 'elsif', 'status', '==', ':timed_out', 'heading', '(', '\"Result: \"', ',', 'status_string', ',', ':yellow', ')', 'level', '=', ':fatal', 'else', 'heading', '(', '\"Result: \"', ',', 'status_string', ',', ':red', ')', 'level', '=', ':fatal', 'end', 'if', '(', 'actions_sentence', '=', 'summary', '.', 'actions_sentence', '.', 'presence', ')', 'public_send', '(', 'level', ',', 'actions_sentence', ')', 'blank_line', '(', 'level', ')', 'end', 'summary', '.', 'paragraphs', '.', 'each', 'do', '|', 'para', '|', 'msg_lines', '=', 'para', '.', 'split', '(', '\"\\\\n\"', ')', 'msg_lines', '.', 'each', '{', '|', 'line', '|', 'public_send', '(', 'level', ',', 'line', ')', '}', 'blank_line', '(', 'level', ')', 'unless', 'para', '==', 'summary', '.', 'paragraphs', '.', 'last', 'end', 'end']' (1291166181.py:27, <module>())\u001B[0m\n",
      "\u001B[32m[2021-11-27 23:37:16]    INFO >> After BPE, '['def', 'Ġprint', '_', 'summary', 'Ġ(', 'Ġstatus', 'Ġ)', 'Ġstatus', '_', 'string', 'Ġ=', 'Ġstatus', 'Ġ.', 'Ġto', '_', 's', 'Ġ.', 'Ġhuman', 'ize', 'Ġ.', 'Ġup', 'case', 'Ġif', 'Ġstatus', 'Ġ==', 'Ġ:', 'success', 'Ġheading', 'Ġ(', 'Ġ\"', 'Result', ':', 'Ġ\"', 'Ġ,', 'Ġstatus', '_', 'string', 'Ġ,', 'Ġ:', 'green', 'Ġ)', 'Ġlevel', 'Ġ=', 'Ġ:', 'info', 'Ġel', 's', 'if', 'Ġstatus', 'Ġ==', 'Ġ:', 'tim', 'ed', '_', 'out', 'Ġheading', 'Ġ(', 'Ġ\"', 'Result', ':', 'Ġ\"', 'Ġ,', 'Ġstatus', '_', 'string', 'Ġ,', 'Ġ:', 'yellow', 'Ġ)', 'Ġlevel', 'Ġ=', 'Ġ:', 'f', 'atal', 'Ġelse', 'Ġheading', 'Ġ(', 'Ġ\"', 'Result', ':', 'Ġ\"', 'Ġ,', 'Ġstatus', '_', 'string', 'Ġ,', 'Ġ:', 'red', 'Ġ)', 'Ġlevel', 'Ġ=', 'Ġ:', 'f', 'atal', 'Ġend', 'Ġif', 'Ġ(', 'Ġactions', '_', 'sent', 'ence', 'Ġ=', 'Ġsummary', 'Ġ.', 'Ġactions', '_', 'sent', 'ence', 'Ġ.', 'Ġpresence', 'Ġ)', 'Ġpublic', '_', 'send', 'Ġ(', 'Ġlevel', 'Ġ,', 'Ġactions', '_', 'sent', 'ence', 'Ġ)', 'Ġblank', '_', 'line', 'Ġ(', 'Ġlevel', 'Ġ)', 'Ġend', 'Ġsummary', 'Ġ.', 'Ġparagraphs', 'Ġ.', 'Ġeach', 'Ġdo', 'Ġ|', 'Ġpara', 'Ġ|', 'Ġmsg', '_', 'lines', 'Ġ=', 'Ġpara', 'Ġ.', 'Ġsplit', 'Ġ(', 'Ġ\"\\\\', 'n', '\"', 'Ġ)', 'Ġmsg', '_', 'lines', 'Ġ.', 'Ġeach', 'Ġ{', 'Ġ|', 'Ġline', 'Ġ|', 'Ġpublic', '_', 'send', 'Ġ(', 'Ġlevel', 'Ġ,', 'Ġline', 'Ġ)', 'Ġ}', 'Ġblank', '_', 'line', 'Ġ(', 'Ġlevel', 'Ġ)', 'Ġunless', 'Ġpara', 'Ġ==', 'Ġsummary', 'Ġ.', 'Ġparagraphs', 'Ġ.', 'Ġlast', 'Ġend', 'Ġend']' (1291166181.py:28, <module>())\u001B[0m\n",
      "  6%|██▎                                     | 73/1261 [00:00<00:03, 367.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 1261/1261 [00:03<00:00, 412.95it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import ujson\n",
    "from tqdm import tqdm\n",
    "from ncc.data.indexed_dataset import MMapIndexedDatasetBuilder\n",
    "\n",
    "\n",
    "def total_lines(reader):\n",
    "    num = sum(1 for _ in reader)\n",
    "    reader.seek(0)\n",
    "    return num\n",
    "\n",
    "\n",
    "SRC_DIR = os.path.join(DATASET_DIR, \"CodeSearchNet\", \"ruby\")\n",
    "for mode in [\"train\", \"valid\", \"test\"]:\n",
    "    SRC_FILE = os.path.join(SRC_DIR, f\"{mode}.jsonl\")\n",
    "    DST_FILE = os.path.join(SRC_DIR, mode)\n",
    "    mmap_dataset_builder = MMapIndexedDatasetBuilder(f\"{DST_FILE}.mmap\")\n",
    "\n",
    "    with open(SRC_FILE, 'r') as reader:\n",
    "        for idx, code_snippet in enumerate(tqdm(reader, total=total_lines(reader))):\n",
    "            code_snippet = ujson.loads(code_snippet)\n",
    "            raw_code_tokens = code_snippet['code_tokens']\n",
    "            after_code_tokens = vocab.subtokenize(raw_code_tokens)\n",
    "            if idx == 0:\n",
    "                print()\n",
    "                LOGGER.info(f\"Show a case of {SRC_FILE}\")\n",
    "                LOGGER.info(f\"Before BPE, '{raw_code_tokens}'\")\n",
    "                LOGGER.info(f\"After BPE, '{after_code_tokens}'\")\n",
    "            tensor = torch.IntTensor(vocab.tokens_to_indices(after_code_tokens))\n",
    "            mmap_dataset_builder.add_item(tensor)\n",
    "    mmap_dataset_builder.finalize(f\"{DST_FILE}.idx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Step 3. Design your model in NaturalCC <br>\n",
    "*You should ensure task, model, dataset meet your requirements.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 1) design your dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from ncc.data.ncc_dataset import NccDataset\n",
    "\n",
    "\n",
    "def collate(samples, pad_idx, ):\n",
    "    from ncc.data.tools import data_utils\n",
    "    def merge(key):\n",
    "        return data_utils.collate_tokens(\n",
    "            [s[key] for s in samples],\n",
    "            pad_idx,\n",
    "        )\n",
    "\n",
    "    src_tokens = merge('source')\n",
    "    tgt_tokens = merge('target')\n",
    "    return {\n",
    "        'id': [s['id'] for s in samples],\n",
    "        'net_input': {\n",
    "            'src_tokens': src_tokens,\n",
    "        },\n",
    "        'target': tgt_tokens,\n",
    "    }\n",
    "\n",
    "\n",
    "class DemoDataset(NccDataset):\n",
    "    def __init__(self, dict, data, sizes):\n",
    "        self.dict = dict\n",
    "        self.data = data\n",
    "        self.sizes = sizes\n",
    "        self.pad = dict.pad()\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        src_item = self.data[index][:-1]\n",
    "        tgt_item = self.data[index][1:]\n",
    "        example = {\n",
    "            'id': index,\n",
    "            'source': src_item,\n",
    "            'target': tgt_item,\n",
    "        }\n",
    "        return example\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def ordered_indices(self):\n",
    "        import numpy as np\n",
    "        return np.random.permutation(len(self))\n",
    "\n",
    "    def collater(self, samples):\n",
    "        return collate(samples, pad_idx=self.pad)\n",
    "\n",
    "    def num_tokens(self, index):\n",
    "        # Return the number of tokens in a sample.\n",
    "        return self.sizes[index]\n",
    "\n",
    "    def size(self, index):\n",
    "        # Return an example's size.\n",
    "        return self.sizes[index]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) register your task\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "from ncc.tasks import NccTask, register_task\n",
    "\n",
    "\n",
    "@register_task('demo')\n",
    "class DemoTask(NccTask):\n",
    "    def __init__(self, dictionary):\n",
    "        super(DemoTask, self).__init__(args=None)\n",
    "        self.dictionary = dictionary\n",
    "\n",
    "    def load_dataset(self, split, data_file):\n",
    "        # define your loading rules\n",
    "        from ncc.data.indexed_dataset import MMapIndexedDataset\n",
    "        from ncc.data.wrappers import TruncateDataset\n",
    "        # truncate code with a length of 128 + 1\n",
    "        dataset = TruncateDataset(\n",
    "            MMapIndexedDataset(data_file),\n",
    "            truncation_length=128 + 1,\n",
    "        )\n",
    "        datasizes = dataset.sizes\n",
    "        self.datasets[split] = DemoDataset(self.dictionary, dataset, datasizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 3) register your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from ncc.models import register_model\n",
    "from ncc.modules.base.layers import (\n",
    "    Embedding, Linear, LSTM\n",
    ")\n",
    "from ncc.models.ncc_model import NccLanguageModel\n",
    "\n",
    "\n",
    "@register_model(\"demo\")\n",
    "class DemoModel(NccLanguageModel):\n",
    "    def __init__(self, dictionary, decoder):\n",
    "        super().__init__(decoder)\n",
    "        self.dictionary = dictionary\n",
    "\n",
    "    @classmethod\n",
    "    def build_model(cls, dictionary):\n",
    "        from ncc.modules.decoders.ncc_decoder import NccDecoder\n",
    "\n",
    "        class DemoDecoder(NccDecoder):\n",
    "            def __init__(self, dictionary):\n",
    "                super(DemoDecoder, self).__init__(dictionary)\n",
    "                self.embedding = Embedding(len(dictionary), embedding_dim=512, padding_idx=dictionary.pad())\n",
    "                self.out_projector = Linear(512, len(dictionary))\n",
    "                # share embedding weight\n",
    "                self.out_projector.weight = self.embedding.weight\n",
    "                self.lstm = LSTM(512, 512)\n",
    "\n",
    "            def forward(self, src_tokens, **kwargs):\n",
    "                x = self.embedding(src_tokens)  # B, L-1, E\n",
    "                x, _ = self.lstm(x)\n",
    "                x = self.out_projector(x)\n",
    "                return x\n",
    "\n",
    "        decoder = DemoDecoder(dictionary)\n",
    "        return cls(dictionary, decoder=decoder)\n",
    "\n",
    "    def forward(self, src_tokens, **kwargs):\n",
    "        return self.decoder.forward(src_tokens, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 4) load datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "task = DemoTask(dictionary=vocab)\n",
    "for mode in [\"train\", \"valid\", \"test\"]:\n",
    "    task.load_dataset(split=mode, data_file=os.path.join(SRC_DIR, \"train\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4. Train & inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yang/Github/naturalcc-dev/ncc/data/indexed_dataset/mmap_indexed_dataset.py:181: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1614378124864/work/torch/csrc/utils/tensor_numpy.cpp:143.)\n",
      "  return torch.from_numpy(np_array)\n",
      "\u001B[32m[2021-11-27 23:37:22]    INFO >> CrossEntropy loss: 10.8253 (829832851.py:30, <module>())\u001B[0m\n",
      "\u001B[32m[2021-11-27 23:37:23]    INFO >> CrossEntropy loss: 10.8194 (829832851.py:30, <module>())\u001B[0m\n",
      "\u001B[32m[2021-11-27 23:37:23]    INFO >> CrossEntropy loss: 10.7962 (829832851.py:30, <module>())\u001B[0m\n",
      "\u001B[32m[2021-11-27 23:37:23]    INFO >> CrossEntropy loss: 10.7495 (829832851.py:30, <module>())\u001B[0m\n",
      "\u001B[32m[2021-11-27 23:37:23]    INFO >> CrossEntropy loss: 10.7122 (829832851.py:30, <module>())\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = DemoModel.build_model(dictionary=vocab)\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "\n",
    "from torch.optim import Adam\n",
    "\n",
    "optimizer = Adam(lr=1e-3, params=model.parameters())\n",
    "\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from ncc.utils.utils import move_to_cuda\n",
    "\n",
    "# train\n",
    "BATCH_SIZE = 4\n",
    "train_iter = task.get_batch_iterator(dataset=task.dataset(\"train\"), max_sentences=BATCH_SIZE). \\\n",
    "    next_epoch_itr(shuffle=True)\n",
    "model.train()\n",
    "for idx in range(5):\n",
    "    batch = next(train_iter)\n",
    "    if torch.cuda.is_available():\n",
    "        batch = move_to_cuda(batch)\n",
    "    logits = model.forward(**batch['net_input'])\n",
    "    lprobs = torch.log_softmax(logits, dim=-1).view(-1, logits.size(-1))\n",
    "    golds = batch['target'].view(-1)\n",
    "    # ignore pad\n",
    "    loss = F.nll_loss(lprobs, golds, ignore_index=vocab.pad())\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    LOGGER.info(f\"CrossEntropy loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 2) inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[2021-11-27 23:37:23]    INFO >> MRR@10: 10.7122 (4291989905.py:19, <module>())\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 4\n",
    "test_iter = task.get_batch_iterator(dataset=task.dataset(\"test\"), max_sentences=BATCH_SIZE). \\\n",
    "    next_epoch_itr(shuffle=False)\n",
    "model.eval()\n",
    "batch = next(test_iter)\n",
    "if torch.cuda.is_available():\n",
    "    batch = move_to_cuda(batch)\n",
    "logits = model.forward(**batch['net_input'])\n",
    "# ignore pad\n",
    "valid_indices = batch['net_input']['src_tokens'].view(-1) != task.dictionary.pad()\n",
    "lprobs = torch.log_softmax(logits, dim=-1).view(-1, logits.size(-1))\n",
    "lprobs = lprobs[valid_indices]\n",
    "golds = batch['target'].view(-1)\n",
    "golds = golds[valid_indices]\n",
    "ranks = (lprobs >= lprobs[:, golds].diag().unsqueeze(dim=-1)).sum(-1)\n",
    "mrr = 1. / ranks\n",
    "mrr[ranks > 10] = 0.\n",
    "mrr = mrr.sum().float().item()\n",
    "LOGGER.info(f\"MRR@10: {loss.item():.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}