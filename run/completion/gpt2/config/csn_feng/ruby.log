nohup: ignoring input
[2021-04-04 04:45:53]    INFO >> Load arguments in /home/wanyao/yang/naturalcc-dev/run/completion/gpt2/config/csn_feng/ruby.yml (train.py:291, cli_main())
[2021-04-04 04:45:53]    INFO >> {'criterion': 'completion_cross_entropy', 'optimizer': 'torch_adam', 'lr_scheduler': 'fixed', 'tokenizer': None, 'bpe': None, 'common': {'no_progress_bar': 0, 'log_interval': 500, 'log_format': 'simple', 'tensorboard_logdir': '', 'memory_efficient_fp16': 0, 'fp16_no_flatten_grads': 0, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'empty_cache_freq': 0, 'task': 'completion', 'seed': 666, 'cpu': 0, 'fp16': 0, 'fp16_opt_level': '01', 'server_ip': '', 'server_port': ''}, 'dataset': {'num_workers': 3, 'skip_invalid_size_inputs_valid_test': 0, 'max_tokens': 100000.0, 'max_sentences': 32, 'required_batch_size_multiple': 1, 'dataset_impl': 'mmap', 'train_subset': 'train', 'valid_subset': 'valid', 'validate_interval': 1, 'fixed_validation_seed': None, 'disable_validation': 0, 'max_tokens_valid': None, 'max_sentences_valid': 64, 'curriculum': 5, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'truncate_target': 1}, 'distributed_training': {'distributed_world_size': 4, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': 0, 'ddp_backend': 'no_c10d', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': None, 'find_unused_parameters': 0, 'fast_stat_sync': 0, 'broadcast_buffers': 0, 'global_sync_iter': 50, 'warmup_iterations': 500, 'local_rank': -1}, 'task': {'data': '/mnt/wanyao/ncc_data/codexglue/code_to_text/completion/data-mmap/ruby', 'target_lang': 'code_tokens', 'max_target_positions': 513, 'add_bos_token': 0, 'eval_bleu': 0, 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': None, 'eval_tokenized_bleu': 0, 'eval_bleu_remove_bpe': None, 'eval_bleu_args': None, 'eval_bleu_print_samples': 0, 'eval_mrr': 1}, 'model': {'arch': 'completion_gpt2', 'dropout': 0.5, 'decoder_embed_dim': 300, 'decoder_hidden_size': 300, 'decoder_layers': 6, 'decoder_attention_heads': 6, 'max_target_positions': 513, 'activation_fn': 'gelu', 'decoder_ffn_embed_dim': 1200}, 'optimization': {'max_epoch': 100, 'max_update': 0, 'clip_norm': 25, 'update_freq': [1], 'lrs': [0.001], 'min_lr': -1, 'use_bmuf': 0, 'force_anneal': None, 'warmup_updates': 0, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 1000000, 'sentence_avg': None, 'adam': {'adam_betas': '(0.9, 0.999)', 'adam_eps': 1e-08, 'weight_decay': 0.0, 'use_old_adam': 0}, 'weight_decay': 0.0, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 5, 'max_steps': -1, 'warmup_steps': 0, 'gradient_accumulation_steps': 1}, 'checkpoint': {'save_dir': '/mnt/wanyao/ncc_data/codexglue/code_to_text/completion/data-mmap/ruby/gpt2/checkpoints', 'restore_file': 'checkpoint_best.pt', 'reset_dataloader': None, 'reset_lr_scheduler': None, 'reset_meters': None, 'reset_optimizer': None, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': 0, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': 0, 'no_epoch_checkpoints': 1, 'no_last_checkpoints': 0, 'no_save_optimizer_state': None, 'best_checkpoint_metric': 'mrr', 'maximize_best_checkpoint_metric': 1, 'patience': 5, 'should_continue': 0, 'model_name_or_path': None, 'cache_dir': None, 'logging_steps': 500, 'save_steps': 2000, 'save_total_limit': 2, 'overwrite_output_dir': 0, 'overwrite_cache': 0}, 'eval': {'path': '/mnt/wanyao/ncc_data/codexglue/code_to_text/completion/data-mmap/ruby/gpt2/checkpoints/checkpoint_best.pt', 'model_overrides': '{}', 'checkpoint_suffix': '', 'max_sentences_eval': 64}} (train.py:293, cli_main())
[2021-04-04 04:45:55]    INFO >> distributed init (rank 0): tcp://localhost:18666 (distributed_utils.py:84, distributed_init())
[2021-04-04 04:45:55]    INFO >> distributed init (rank 2): tcp://localhost:18666 (distributed_utils.py:84, distributed_init())
[2021-04-04 04:45:55]    INFO >> Added key: store_based_barrier_key:1 to store for rank: 2 (distributed_c10d.py:187, _store_based_barrier())
[2021-04-04 04:45:55]    INFO >> distributed init (rank 3): tcp://localhost:18666 (distributed_utils.py:84, distributed_init())
[2021-04-04 04:45:55]    INFO >> Added key: store_based_barrier_key:1 to store for rank: 3 (distributed_c10d.py:187, _store_based_barrier())
[2021-04-04 04:45:55]    INFO >> distributed init (rank 1): tcp://localhost:18666 (distributed_utils.py:84, distributed_init())
[2021-04-04 04:45:55]    INFO >> Added key: store_based_barrier_key:1 to store for rank: 1 (distributed_c10d.py:187, _store_based_barrier())
[2021-04-04 04:45:55]    INFO >> Added key: store_based_barrier_key:1 to store for rank: 0 (distributed_c10d.py:187, _store_based_barrier())
[2021-04-04 04:45:55]    INFO >> initialized host node14 as rank 0 (distributed_utils.py:93, distributed_init())
[2021-04-04 04:45:55]    INFO >> initialized host node14 as rank 3 (distributed_utils.py:93, distributed_init())
[2021-04-04 04:45:55]    INFO >> initialized host node14 as rank 1 (distributed_utils.py:93, distributed_init())
[2021-04-04 04:45:55]    INFO >> initialized host node14 as rank 2 (distributed_utils.py:93, distributed_init())
[2021-04-04 04:46:01]    INFO >> {'criterion': 'completion_cross_entropy', 'optimizer': 'torch_adam', 'lr_scheduler': 'fixed', 'tokenizer': None, 'bpe': None, 'common': {'no_progress_bar': 0, 'log_interval': 500, 'log_format': 'simple', 'tensorboard_logdir': '', 'memory_efficient_fp16': 0, 'fp16_no_flatten_grads': 0, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'empty_cache_freq': 0, 'task': 'completion', 'seed': 666, 'cpu': 0, 'fp16': 0, 'fp16_opt_level': '01', 'server_ip': '', 'server_port': ''}, 'dataset': {'num_workers': 3, 'skip_invalid_size_inputs_valid_test': 0, 'max_tokens': 100000.0, 'max_sentences': 32, 'required_batch_size_multiple': 1, 'dataset_impl': 'mmap', 'train_subset': 'train', 'valid_subset': 'valid', 'validate_interval': 1, 'fixed_validation_seed': None, 'disable_validation': 0, 'max_tokens_valid': None, 'max_sentences_valid': 64, 'curriculum': 5, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'truncate_target': 1}, 'distributed_training': {'distributed_world_size': 4, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:18666', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': 0, 'ddp_backend': 'no_c10d', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': None, 'find_unused_parameters': 0, 'fast_stat_sync': 0, 'broadcast_buffers': 0, 'global_sync_iter': 50, 'warmup_iterations': 500, 'local_rank': -1}, 'task': {'data': '/mnt/wanyao/ncc_data/codexglue/code_to_text/completion/data-mmap/ruby', 'target_lang': 'code_tokens', 'max_target_positions': 513, 'add_bos_token': 0, 'eval_bleu': 0, 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': None, 'eval_tokenized_bleu': 0, 'eval_bleu_remove_bpe': None, 'eval_bleu_args': None, 'eval_bleu_print_samples': 0, 'eval_mrr': 1}, 'model': {'arch': 'completion_gpt2', 'dropout': 0.5, 'decoder_embed_dim': 300, 'decoder_hidden_size': 300, 'decoder_layers': 6, 'decoder_attention_heads': 6, 'max_target_positions': 513, 'activation_fn': 'gelu', 'decoder_ffn_embed_dim': 1200}, 'optimization': {'max_epoch': 100, 'max_update': 0, 'clip_norm': 25, 'update_freq': [1], 'lrs': [0.001], 'min_lr': -1, 'use_bmuf': 0, 'force_anneal': None, 'warmup_updates': 0, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 1000000, 'sentence_avg': None, 'adam': {'adam_betas': '(0.9, 0.999)', 'adam_eps': 1e-08, 'weight_decay': 0.0, 'use_old_adam': 0}, 'weight_decay': 0.0, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 5, 'max_steps': -1, 'warmup_steps': 0, 'gradient_accumulation_steps': 1}, 'checkpoint': {'save_dir': '/mnt/wanyao/ncc_data/codexglue/code_to_text/completion/data-mmap/ruby/gpt2/checkpoints', 'restore_file': 'checkpoint_best.pt', 'reset_dataloader': None, 'reset_lr_scheduler': None, 'reset_meters': None, 'reset_optimizer': None, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': 0, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': 0, 'no_epoch_checkpoints': 1, 'no_last_checkpoints': 0, 'no_save_optimizer_state': None, 'best_checkpoint_metric': 'mrr', 'maximize_best_checkpoint_metric': 1, 'patience': 5, 'should_continue': 0, 'model_name_or_path': None, 'cache_dir': None, 'logging_steps': 500, 'save_steps': 2000, 'save_total_limit': 2, 'overwrite_output_dir': 0, 'overwrite_cache': 0}, 'eval': {'path': '/mnt/wanyao/ncc_data/codexglue/code_to_text/completion/data-mmap/ruby/gpt2/checkpoints/checkpoint_best.pt', 'model_overrides': '{}', 'checkpoint_suffix': '', 'max_sentences_eval': 64}} (train.py:201, single_main())
[2021-04-04 04:46:01]    INFO >> [code_tokens] dictionary: 50000 types (completion.py:101, setup_task())
[2021-04-04 04:46:01]    INFO >> Truncate dataset into max length: 513 (completion.py:41, load_token_dataset())
[2021-04-04 04:46:01]    INFO >> loaded 1400 examples from: /mnt/wanyao/ncc_data/codexglue/code_to_text/completion/data-mmap/ruby/valid.code_tokens (completion.py:42, load_token_dataset())
[2021-04-04 04:46:01]    INFO >> GPT2(
  (decoder): TransformerDecoder(
    (embed_tokens): Embedding(50000, 300, padding_idx=0)
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (in_layer_norm): LayerNorm((300,), eps=1e-08, elementwise_affine=True)
        (attention): MultiheadAttention(
          (k_proj): Linear(in_features=300, out_features=300, bias=True)
          (v_proj): Linear(in_features=300, out_features=300, bias=True)
          (q_proj): Linear(in_features=300, out_features=300, bias=True)
          (out_proj): Linear(in_features=300, out_features=300, bias=True)
        )
        (ff_layer_norm): LayerNorm((300,), eps=1e-08, elementwise_affine=True)
        (fc1): Linear(in_features=300, out_features=1200, bias=True)
        (fc2): Linear(in_features=1200, out_features=300, bias=True)
      )
      (1): TransformerDecoderLayer(
        (in_layer_norm): LayerNorm((300,), eps=1e-08, elementwise_affine=True)
        (attention): MultiheadAttention(
          (k_proj): Linear(in_features=300, out_features=300, bias=True)
          (v_proj): Linear(in_features=300, out_features=300, bias=True)
          (q_proj): Linear(in_features=300, out_features=300, bias=True)
          (out_proj): Linear(in_features=300, out_features=300, bias=True)
        )
        (ff_layer_norm): LayerNorm((300,), eps=1e-08, elementwise_affine=True)
        (fc1): Linear(in_features=300, out_features=1200, bias=True)
        (fc2): Linear(in_features=1200, out_features=300, bias=True)
      )
      (2): TransformerDecoderLayer(
        (in_layer_norm): LayerNorm((300,), eps=1e-08, elementwise_affine=True)
        (attention): MultiheadAttention(
          (k_proj): Linear(in_features=300, out_features=300, bias=True)
          (v_proj): Linear(in_features=300, out_features=300, bias=True)
          (q_proj): Linear(in_features=300, out_features=300, bias=True)
          (out_proj): Linear(in_features=300, out_features=300, bias=True)
        )
        (ff_layer_norm): LayerNorm((300,), eps=1e-08, elementwise_affine=True)
        (fc1): Linear(in_features=300, out_features=1200, bias=True)
        (fc2): Linear(in_features=1200, out_features=300, bias=True)
      )
      (3): TransformerDecoderLayer(
        (in_layer_norm): LayerNorm((300,), eps=1e-08, elementwise_affine=True)
        (attention): MultiheadAttention(
          (k_proj): Linear(in_features=300, out_features=300, bias=True)
          (v_proj): Linear(in_features=300, out_features=300, bias=True)
          (q_proj): Linear(in_features=300, out_features=300, bias=True)
          (out_proj): Linear(in_features=300, out_features=300, bias=True)
        )
        (ff_layer_norm): LayerNorm((300,), eps=1e-08, elementwise_affine=True)
        (fc1): Linear(in_features=300, out_features=1200, bias=True)
        (fc2): Linear(in_features=1200, out_features=300, bias=True)
      )
      (4): TransformerDecoderLayer(
        (in_layer_norm): LayerNorm((300,), eps=1e-08, elementwise_affine=True)
        (attention): MultiheadAttention(
          (k_proj): Linear(in_features=300, out_features=300, bias=True)
          (v_proj): Linear(in_features=300, out_features=300, bias=True)
          (q_proj): Linear(in_features=300, out_features=300, bias=True)
          (out_proj): Linear(in_features=300, out_features=300, bias=True)
        )
        (ff_layer_norm): LayerNorm((300,), eps=1e-08, elementwise_affine=True)
        (fc1): Linear(in_features=300, out_features=1200, bias=True)
        (fc2): Linear(in_features=1200, out_features=300, bias=True)
      )
      (5): TransformerDecoderLayer(
        (in_layer_norm): LayerNorm((300,), eps=1e-08, elementwise_affine=True)
        (attention): MultiheadAttention(
          (k_proj): Linear(in_features=300, out_features=300, bias=True)
          (v_proj): Linear(in_features=300, out_features=300, bias=True)
          (q_proj): Linear(in_features=300, out_features=300, bias=True)
          (out_proj): Linear(in_features=300, out_features=300, bias=True)
        )
        (ff_layer_norm): LayerNorm((300,), eps=1e-08, elementwise_affine=True)
        (fc1): Linear(in_features=300, out_features=1200, bias=True)
        (fc2): Linear(in_features=1200, out_features=300, bias=True)
      )
    )
    (out_layer_norm): LayerNorm((300,), eps=1e-08, elementwise_affine=True)
  )
) (train.py:212, single_main())
[2021-04-04 04:46:01]    INFO >> model completion_gpt2, criterion CompletionCrossEntropyCriterion (train.py:213, single_main())
[2021-04-04 04:46:01]    INFO >> num. model params: 23076864 (num. trained: 23076864) (train.py:214, single_main())
[2021-04-04 04:46:01]    INFO >> training on 4 GPUs (train.py:221, single_main())
[2021-04-04 04:46:01]    INFO >> max tokens per GPU = 100000.0 and max sentences per GPU = 32 (train.py:222, single_main())
[2021-04-04 04:46:01]    INFO >> no existing checkpoint found checkpoint_best.pt (ncc_trainers.py:270, load_checkpoint())
[2021-04-04 04:46:01]    INFO >> loading train data for epoch 1 (ncc_trainers.py:284, get_train_iterator())
[2021-04-04 04:46:01]    INFO >> Truncate dataset into max length: 513 (completion.py:41, load_token_dataset())
[2021-04-04 04:46:01]    INFO >> loaded 24927 examples from: /mnt/wanyao/ncc_data/codexglue/code_to_text/completion/data-mmap/ruby/train.code_tokens (completion.py:42, load_token_dataset())
[2021-04-04 04:46:02]    INFO >> NOTE: your device may support faster training with fp16 (ncc_trainers.py:155, _setup_optimizer())
/home/wanyao/yang/naturalcc-dev/ncc/utils/gradient_clip/fairseq_clip.py:56: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  warnings.warn(
[2021-04-04 04:46:54]    INFO >> epoch 001 | loss 5.988 | accuracy 0 | mrr 0 | ppl 63.49 | wps 34275.6 | ups 4.26 | wpb 8045.7 | bsz 127.8 | num_updates 195 | lr 0.001 | gnorm 0.959 | clip 0 | train_wall 45 | wall 52 (progress_bar.py:269, print())
/home/wanyao/yang/naturalcc-dev/ncc/utils/gradient_clip/fairseq_clip.py:56: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  warnings.warn(
/home/wanyao/yang/naturalcc-dev/ncc/utils/gradient_clip/fairseq_clip.py:56: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  warnings.warn(
/home/wanyao/yang/naturalcc-dev/ncc/utils/gradient_clip/fairseq_clip.py:56: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  warnings.warn(
[2021-04-04 04:47:04]    INFO >> epoch 001 | valid on 'valid' subset | loss 5.023 | accuracy 0.378 | mrr 0.495644 | ppl 32.51 | wps 19023.7 | wpb 14868.2 | bsz 233.3 | num_updates 195 (progress_bar.py:269, print())
[2021-04-04 04:47:05]    INFO >> saved checkpoint /mnt/wanyao/ncc_data/codexglue/code_to_text/completion/data-mmap/ruby/gpt2/checkpoints/checkpoint_best.pt (epoch 1 @ 195 updates, score 0.495644) (writing took 0.998654 seconds) (checkpoint_utils.py:79, save_checkpoint())
[2021-04-04 04:47:55]    INFO >> epoch 002 | loss 4.528 | accuracy 0 | mrr 0 | ppl 23.07 | wps 25495.1 | ups 3.17 | wpb 8045.7 | bsz 127.8 | num_updates 390 | lr 0.001 | gnorm 0.785 | clip 0 | train_wall 43 | wall 114 (progress_bar.py:269, print())
[2021-04-04 04:48:03]    INFO >> epoch 002 | valid on 'valid' subset | loss 4.431 | accuracy 0.421594 | mrr 0.545942 | ppl 21.57 | wps 33909.6 | wpb 14868.2 | bsz 233.3 | num_updates 390 | best_mrr 0.545942 (progress_bar.py:269, print())
[2021-04-04 04:48:17]    INFO >> saved checkpoint /mnt/wanyao/ncc_data/codexglue/code_to_text/completion/data-mmap/ruby/gpt2/checkpoints/checkpoint_best.pt (epoch 2 @ 390 updates, score 0.545942) (writing took 14.138967 seconds) (checkpoint_utils.py:79, save_checkpoint())
[2021-04-04 04:48:47]    INFO >> epoch 003:    110 / 195 loss=4.983, accuracy=0, mrr=0, ppl=31.61, wps=25196.2, ups=3.13, wpb=8044.8, bsz=127.9, num_updates=500, lr=0.001, gnorm=0.847, clip=0, train_wall=113, wall=166 (progress_bar.py:260, log())
[2021-04-04 04:49:07]    INFO >> epoch 003 | loss 3.886 | accuracy 0 | mrr 0 | ppl 14.79 | wps 21796.8 | ups 2.71 | wpb 8045.7 | bsz 127.8 | num_updates 585 | lr 0.001 | gnorm 0.77 | clip 0 | train_wall 43 | wall 186 (progress_bar.py:269, print())
[2021-04-04 04:49:15]    INFO >> epoch 003 | valid on 'valid' subset | loss 4.129 | accuracy 0.445415 | mrr 0.57027 | ppl 17.49 | wps 34195.3 | wpb 14868.2 | bsz 233.3 | num_updates 585 | best_mrr 0.57027 (progress_bar.py:269, print())
[2021-04-04 04:49:29]    INFO >> saved checkpoint /mnt/wanyao/ncc_data/codexglue/code_to_text/completion/data-mmap/ruby/gpt2/checkpoints/checkpoint_best.pt (epoch 3 @ 585 updates, score 0.57027) (writing took 13.843040 seconds) (checkpoint_utils.py:79, save_checkpoint())
[2021-04-04 04:50:19]    INFO >> epoch 004 | loss 3.44 | accuracy 0 | mrr 0 | ppl 10.85 | wps 21915.2 | ups 2.72 | wpb 8045.7 | bsz 127.8 | num_updates 780 | lr 0.001 | gnorm 0.775 | clip 0 | train_wall 43 | wall 257 (progress_bar.py:269, print())
[2021-04-04 04:50:27]    INFO >> epoch 004 | valid on 'valid' subset | loss 4 | accuracy 0.456456 | mrr 0.581186 | ppl 16 | wps 34884.7 | wpb 14868.2 | bsz 233.3 | num_updates 780 | best_mrr 0.581186 (progress_bar.py:269, print())
[2021-04-04 04:50:41]    INFO >> saved checkpoint /mnt/wanyao/ncc_data/codexglue/code_to_text/completion/data-mmap/ruby/gpt2/checkpoints/checkpoint_best.pt (epoch 4 @ 780 updates, score 0.581186) (writing took 14.087241 seconds) (checkpoint_utils.py:79, save_checkpoint())
[2021-04-04 04:51:31]    INFO >> epoch 005 | loss 3.123 | accuracy 0 | mrr 0 | ppl 8.71 | wps 21685.7 | ups 2.7 | wpb 8045.7 | bsz 127.8 | num_updates 975 | lr 0.001 | gnorm 0.788 | clip 0 | train_wall 43 | wall 330 (progress_bar.py:269, print())
[2021-04-04 04:51:39]    INFO >> epoch 005 | valid on 'valid' subset | loss 3.951 | accuracy 0.463373 | mrr 0.586831 | ppl 15.47 | wps 33858.1 | wpb 14868.2 | bsz 233.3 | num_updates 975 | best_mrr 0.586831 (progress_bar.py:269, print())
[2021-04-04 04:51:53]    INFO >> saved checkpoint /mnt/wanyao/ncc_data/codexglue/code_to_text/completion/data-mmap/ruby/gpt2/checkpoints/checkpoint_best.pt (epoch 5 @ 975 updates, score 0.586831) (writing took 14.027428 seconds) (checkpoint_utils.py:79, save_checkpoint())
[2021-04-04 04:52:04]    INFO >> epoch 006:     25 / 195 loss=3.338, accuracy=0, mrr=0, ppl=10.11, wps=20408.6, ups=2.54, wpb=8042.6, bsz=127.8, num_updates=1000, lr=0.001, gnorm=0.78, clip=0, train_wall=111, wall=363 (progress_bar.py:260, log())
[2021-04-04 04:52:43]    INFO >> epoch 006 | loss 2.869 | accuracy 0 | mrr 0 | ppl 7.31 | wps 21749.5 | ups 2.7 | wpb 8045.7 | bsz 127.8 | num_updates 1170 | lr 0.001 | gnorm 0.722 | clip 0 | train_wall 43 | wall 402 (progress_bar.py:269, print())
[2021-04-04 04:52:51]    INFO >> epoch 006 | valid on 'valid' subset | loss 3.851 | accuracy 0.473315 | mrr 0.596367 | ppl 14.43 | wps 34587 | wpb 14868.2 | bsz 233.3 | num_updates 1170 | best_mrr 0.596367 (progress_bar.py:269, print())
[2021-04-04 04:53:05]    INFO >> saved checkpoint /mnt/wanyao/ncc_data/codexglue/code_to_text/completion/data-mmap/ruby/gpt2/checkpoints/checkpoint_best.pt (epoch 6 @ 1170 updates, score 0.596367) (writing took 13.929789 seconds) (checkpoint_utils.py:79, save_checkpoint())
[2021-04-04 04:53:56]    INFO >> epoch 007 | loss 2.631 | accuracy 0 | mrr 0 | ppl 6.2 | wps 21676 | ups 2.69 | wpb 8045.7 | bsz 127.8 | num_updates 1365 | lr 0.001 | gnorm 0.733 | clip 0 | train_wall 44 | wall 474 (progress_bar.py:269, print())
[2021-04-04 04:54:04]    INFO >> epoch 007 | valid on 'valid' subset | loss 3.871 | accuracy 0.476062 | mrr 0.597821 | ppl 14.63 | wps 33426.7 | wpb 14868.2 | bsz 233.3 | num_updates 1365 | best_mrr 0.597821 (progress_bar.py:269, print())
[2021-04-04 04:54:18]    INFO >> saved checkpoint /mnt/wanyao/ncc_data/codexglue/code_to_text/completion/data-mmap/ruby/gpt2/checkpoints/checkpoint_best.pt (epoch 7 @ 1365 updates, score 0.597821) (writing took 13.982695 seconds) (checkpoint_utils.py:79, save_checkpoint())
[2021-04-04 04:54:54]    INFO >> epoch 008:    135 / 195 loss=2.655, accuracy=0, mrr=0, ppl=6.3, wps=23747.7, ups=2.95, wpb=8057.1, bsz=127.9, num_updates=1500, lr=0.001, gnorm=0.733, clip=0, train_wall=112, wall=533 (progress_bar.py:260, log())
[2021-04-04 04:55:08]    INFO >> epoch 008 | loss 2.444 | accuracy 0 | mrr 0 | ppl 5.44 | wps 21739.5 | ups 2.7 | wpb 8045.7 | bsz 127.8 | num_updates 1560 | lr 0.001 | gnorm 0.753 | clip 0 | train_wall 43 | wall 546 (progress_bar.py:269, print())
[2021-04-04 04:55:16]    INFO >> epoch 008 | valid on 'valid' subset | loss 3.926 | accuracy 0.473551 | mrr 0.595639 | ppl 15.2 | wps 34681 | wpb 14868.2 | bsz 233.3 | num_updates 1560 | best_mrr 0.597821 (progress_bar.py:269, print())
[2021-04-04 04:55:23]    INFO >> saved checkpoint /mnt/wanyao/ncc_data/codexglue/code_to_text/completion/data-mmap/ruby/gpt2/checkpoints/checkpoint_last.pt (epoch 8 @ 1560 updates, score 0.595639) (writing took 7.503307 seconds) (checkpoint_utils.py:79, save_checkpoint())
[2021-04-04 04:56:13]    INFO >> epoch 009 | loss 2.282 | accuracy 0 | mrr 0 | ppl 4.86 | wps 24042.8 | ups 2.99 | wpb 8045.7 | bsz 127.8 | num_updates 1755 | lr 0.001 | gnorm 0.784 | clip 0 | train_wall 43 | wall 612 (progress_bar.py:269, print())
[2021-04-04 04:56:21]    INFO >> epoch 009 | valid on 'valid' subset | loss 4.023 | accuracy 0.470558 | mrr 0.591192 | ppl 16.25 | wps 35887.1 | wpb 14868.2 | bsz 233.3 | num_updates 1755 | best_mrr 0.597821 (progress_bar.py:269, print())
[2021-04-04 04:56:29]    INFO >> saved checkpoint /mnt/wanyao/ncc_data/codexglue/code_to_text/completion/data-mmap/ruby/gpt2/checkpoints/checkpoint_last.pt (epoch 9 @ 1755 updates, score 0.591192) (writing took 7.768989 seconds) (checkpoint_utils.py:79, save_checkpoint())
[2021-04-04 04:57:19]    INFO >> epoch 010 | loss 2.129 | accuracy 0 | mrr 0 | ppl 4.37 | wps 23616.6 | ups 2.94 | wpb 8045.7 | bsz 127.8 | num_updates 1950 | lr 0.001 | gnorm 0.809 | clip 0 | train_wall 44 | wall 678 (progress_bar.py:269, print())
[2021-04-04 04:57:28]    INFO >> epoch 010 | valid on 'valid' subset | loss 4.121 | accuracy 0.464594 | mrr 0.586051 | ppl 17.4 | wps 34630.2 | wpb 14868.2 | bsz 233.3 | num_updates 1950 | best_mrr 0.597821 (progress_bar.py:269, print())
[2021-04-04 04:57:35]    INFO >> saved checkpoint /mnt/wanyao/ncc_data/codexglue/code_to_text/completion/data-mmap/ruby/gpt2/checkpoints/checkpoint_last.pt (epoch 10 @ 1950 updates, score 0.586051) (writing took 7.591485 seconds) (checkpoint_utils.py:79, save_checkpoint())
[2021-04-04 04:57:52]    INFO >> epoch 011:     50 / 195 loss=2.211, accuracy=0, mrr=0, ppl=4.63, wps=22614.9, ups=2.81, wpb=8045.7, bsz=127.8, num_updates=2000, lr=0.001, gnorm=0.797, clip=0, train_wall=111, wall=711 (progress_bar.py:260, log())
[2021-04-04 04:58:25]    INFO >> epoch 011 | loss 1.986 | accuracy 0 | mrr 0 | ppl 3.96 | wps 24097.3 | ups 3 | wpb 8045.7 | bsz 127.8 | num_updates 2145 | lr 0.001 | gnorm 0.838 | clip 0 | train_wall 43 | wall 743 (progress_bar.py:269, print())
[2021-04-04 04:58:33]    INFO >> epoch 011 | valid on 'valid' subset | loss 4.208 | accuracy 0.466769 | mrr 0.585867 | ppl 18.48 | wps 34440.2 | wpb 14868.2 | bsz 233.3 | num_updates 2145 | best_mrr 0.597821 (progress_bar.py:269, print())
[2021-04-04 04:58:40]    INFO >> saved checkpoint /mnt/wanyao/ncc_data/codexglue/code_to_text/completion/data-mmap/ruby/gpt2/checkpoints/checkpoint_last.pt (epoch 11 @ 2145 updates, score 0.585867) (writing took 7.463833 seconds) (checkpoint_utils.py:79, save_checkpoint())
[2021-04-04 04:59:31]    INFO >> epoch 012 | loss 1.85 | accuracy 0 | mrr 0 | ppl 3.6 | wps 23697.6 | ups 2.95 | wpb 8045.7 | bsz 127.8 | num_updates 2340 | lr 0.001 | gnorm 0.872 | clip 0 | train_wall 44 | wall 809 (progress_bar.py:269, print())
[2021-04-04 04:59:39]    INFO >> epoch 012 | valid on 'valid' subset | loss 4.347 | accuracy 0.457241 | mrr 0.577409 | ppl 20.34 | wps 35216.3 | wpb 14868.2 | bsz 233.3 | num_updates 2340 | best_mrr 0.597821 (progress_bar.py:269, print())
[2021-04-04 04:59:46]    INFO >> saved checkpoint /mnt/wanyao/ncc_data/codexglue/code_to_text/completion/data-mmap/ruby/gpt2/checkpoints/checkpoint_last.pt (epoch 12 @ 2340 updates, score 0.577409) (writing took 7.625506 seconds) (checkpoint_utils.py:79, save_checkpoint())
[2021-04-04 04:59:46]    INFO >> early stop since valid performance hasn't improved for last 5 runs (train.py:176, should_stop_early())
[2021-04-04 04:59:46]    INFO >> early stop since valid performance hasn't improved for last 5 runs (train.py:259, single_main())
[2021-04-04 04:59:46]    INFO >> done training in 824.9 seconds (train.py:271, single_main())
